### CONSTANTS ###
### PULLED FROM LLAMA3 TECHNICAL REPORT ###
MODEL_DIM = 4096
FEED_FORWARD_DIMENSION = 14336
N_TRANSFORMER_BLOCKS = 32
N_ATTENTION_HEADS = 32
RMS_NORM_EPS = 1e-05
ROPE_EMBEDDING_THETA = 500000
HEAD_DIM = MODEL_DIM // N_ATTENTION_HEADS

### LIMITS FOR WORKING ON CPU ###
MAX_BATCH_SIZE = 4
MAX_SEQ_LEN = 128

### FOR GQA ###
N_KV_HEADS = 8
N_KV_HEAD_REPEAT = N_ATTENTION_HEADS // N_KV_HEADS